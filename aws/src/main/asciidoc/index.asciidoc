= Tigase Server Extras - AWS
:author: Tigase Team
:email: team@tigase.net
:toc:
:numbered:

== Overview

Tigase Server Extras - AWS provides you with support for additional features and integrations with Amazon AWS.

== S3 support for HTTP File Upload

By default HTTP File Upload component shipped with Tigase XMPP Server stores uploaded files locally in the directory structure. On AWS it may be better to store data using external service like S3 which are better suited for this task and are more resilient.

=== Enabling storage in S3

To enable storage in S3, you need to add following lines to your configuration file:
[source,dsl]
-----
upload () {
    store (class: tigase.extras.http.upload.S3Store, active: true, exportable: true) {
        bucket = 'bucket-name'
    }
}
-----

This will enable HTTP File Upload component and configure it to be used with S3 bucket named `bucket-name` in the same region as your EC2 instance on which Tigase XMPP Server is running.

WARNING: You would need to manually create this S3 bucket and allow your EC2 instance to access it (read and write). Alternatively, you could add `autocreateBucket = true` inside `store` block, which will enable Tigase XMPP Server to create this S3 bucket in the local AWS region.

If you wish to use S3 bucket from another AWS region, you can do that by adding setting `region` property in the `store` block to the id of the AWS region, ie. set to `us-west-2` to use `US West (Oregon)` region:
[source,dsl]
-----
upload () {
    store (class: tigase.extras.http.upload.S3Store, active: true, exportable: true) {
        bucket = 'bucket-name'
        region = 'us-west-2'
    }
}
-----

If you wish to share the same S3 bucket between different installations of Tigase XMPP Server, you should configure `bucketKeyPrefix` property of `store` with different identifiers for each installation.
That will allow you to easily filter data uploaded for each installation and will allow Tigase XMPP Server to provide you with correct storage usage for each installation.

[source,dsl]
-----
upload () {
    store (class: tigase.extras.http.upload.S3Store, active: true, exportable: true) {
        bucket = 'bucket-name'
        bucketKeyPrefix = '45252AF'
    }
}
-----

== Logging to AWS CloudWatch

To log to AWS CloudWatch there are two modifications required:

1) adding `org.slf4j.bridge.SLF4JBridgeHandler` to `logging` bean:

[source,dsl]
----
logging () {
    rootHandlers = [ 'java.util.logging.ConsoleHandler', 'java.util.logging.FileHandler', 'org.slf4j.bridge.SLF4JBridgeHandler' ]
}
----

2) Add appender configuration to logback.xml configuration file:

[source,xml]
----
<appender name="ASYNC_AWS_LOGS" class="ca.pjer.logback.AwsLogsAppender">

    <!-- Nice layout pattern -->
    <layout>
        <pattern>%d{yyyy-MM-dd HH:mm:ss.SSS} %-5level [%20(%thread)] %logger{5}.%method\(\): %msg %n</pattern>
    </layout>

    <!-- Hardcoded Log Group Name -->
    <logGroupName>/kangaroo-logs/${VHOST}</logGroupName>

    <!-- Log Stream Name UUID Prefix -->
    <logStreamUuidPrefix>${EXTERNAL_IP}/</logStreamUuidPrefix>

    <!-- Hardcoded AWS region -->
    <!-- So even when running inside an AWS instance in us-west-1, logs will go to us-west-2 -->
    <logRegion>us-west-2</logRegion>

    <!-- Maximum number of events in each batch (50 is the default) -->
    <!-- will flush when the event queue has 50 elements, even if still in quiet time (see maxFlushTimeMillis) -->
    <maxBatchLogEvents>50</maxBatchLogEvents>

    <!-- Maximum quiet time in millisecond (0 is the default) -->
    <!-- will flush when met, even if the batch size is not met (see maxBatchLogEvents) -->
    <maxFlushTimeMillis>30000</maxFlushTimeMillis>

    <!-- Maximum block time in millisecond (5000 is the default) -->
    <!-- when > 0: this is the maximum time the logging thread will wait for the logger, -->
    <!-- when == 0: the logging thread will never wait for the logger, discarding events while the queue is full -->
    <maxBlockTimeMillis>5000</maxBlockTimeMillis>

    <!-- Retention value for log groups, 0 for infinite see -->
    <!-- https://docs.aws.amazon.com/AmazonCloudWatchLogs/latest/APIReference/API_PutRetentionPolicy.html for other -->
    <!-- possible values -->

    <retentionTimeDays>60</retentionTimeDays>
</appender>
----

and it to root logger configuration:

[source,xml]
----
<root level="WARN">
    <appender-ref ref="ASYNC_AWS_LOGS"/>
</root>
----